{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-26T18:52:32.559521Z","iopub.execute_input":"2022-06-26T18:52:32.559870Z","iopub.status.idle":"2022-06-26T18:52:32.566099Z","shell.execute_reply.started":"2022-06-26T18:52:32.559839Z","shell.execute_reply":"2022-06-26T18:52:32.565197Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install transformers==4.14.1\n!pip install bitsandbytes-cuda111==0.26.0\n!pip install datasets==1.16.1","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.cuda.amp import custom_fwd, custom_bwd\n\nfrom bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-06-26T18:53:10.452173Z","iopub.execute_input":"2022-06-26T18:53:10.452571Z","iopub.status.idle":"2022-06-26T18:53:16.825006Z","shell.execute_reply.started":"2022-06-26T18:53:10.452531Z","shell.execute_reply":"2022-06-26T18:53:16.824077Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class FrozenBNBLinear(nn.Module):\n    def __init__(self, weight, absmax, code, bias=None):\n        assert isinstance(bias, nn.Parameter) or bias is None\n        super().__init__()\n        self.out_features, self.in_features = weight.shape\n        self.register_buffer(\"weight\", weight.requires_grad_(False))\n        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n        self.register_buffer(\"code\", code.requires_grad_(False))\n        self.adapter = None\n        self.bias = bias\n \n    def forward(self, input):\n        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n        if self.adapter:\n            output += self.adapter(input)\n        return output\n \n    @classmethod\n    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n        return cls(weights_int8, *state, linear.bias)\n \n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n \n \nclass DequantizeAndLinear(torch.autograd.Function): \n    @staticmethod\n    @custom_fwd\n    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n        ctx.save_for_backward(input, weights_quantized, absmax, code)\n        ctx._has_bias = bias is not None\n        return F.linear(input, weights_deq, bias)\n \n    @staticmethod\n    @custom_bwd\n    def backward(ctx, grad_output: torch.Tensor):\n        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n        input, weights_quantized, absmax, code = ctx.saved_tensors\n        # grad_output: [*batch, out_features]\n        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n        grad_input = grad_output @ weights_deq\n        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n        return grad_input, None, None, None, grad_bias\n \n \nclass FrozenBNBEmbedding(nn.Module):\n    def __init__(self, weight, absmax, code):\n        super().__init__()\n        self.num_embeddings, self.embedding_dim = weight.shape\n        self.register_buffer(\"weight\", weight.requires_grad_(False))\n        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n        self.register_buffer(\"code\", code.requires_grad_(False))\n        self.adapter = None\n \n    def forward(self, input, **kwargs):\n        with torch.no_grad():\n            # note: both quantuized weights and input indices are *not* differentiable\n            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n            output = F.embedding(input, weight_deq, **kwargs)\n        if self.adapter:\n            output += self.adapter(input)\n        return output \n \n    @classmethod\n    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n        return cls(weights_int8, *state)\n \n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n \n \ndef quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n    assert chunk_size % 4096 == 0\n    code = None\n    chunks = []\n    absmaxes = []\n    flat_tensor = matrix.view(-1)\n    for i in range((matrix.numel() - 1) // chunk_size + 1):\n        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n        chunks.append(quantized_chunk)\n        absmaxes.append(absmax_chunk)\n \n    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n    absmax = torch.cat(absmaxes)\n    return matrix_i8, (absmax, code)\n \n \ndef convert_to_int8(model):\n    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n    for module in list(model.modules()):\n        for name, child in module.named_children():\n            if isinstance(child, nn.Linear):\n                print(name, child)\n                setattr( \n                    module,\n                    name,\n                    FrozenBNBLinear(\n                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n                        code=torch.zeros(256),\n                        bias=child.bias,\n                    ),\n                )\n            elif isinstance(child, nn.Embedding):\n                setattr(\n                    module,\n                    name,\n                    FrozenBNBEmbedding(\n                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n                        code=torch.zeros(256),\n                    )\n                )","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-26T18:53:16.826521Z","iopub.execute_input":"2022-06-26T18:53:16.827127Z","iopub.status.idle":"2022-06-26T18:53:16.854942Z","shell.execute_reply.started":"2022-06-26T18:53:16.827089Z","shell.execute_reply":"2022-06-26T18:53:16.853722Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n    def __init__(self, config):\n        super().__init__(config)\n\n        convert_to_int8(self.attn)\n        convert_to_int8(self.mlp)\n\n\nclass GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n    def __init__(self, config):\n        super().__init__(config)\n        convert_to_int8(self)\n        \n\nclass GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n    def __init__(self, config):\n        super().__init__(config)\n        convert_to_int8(self)\n\n\ntransformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock  # monkey-patch GPT-J\n\nconfig = transformers.GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6B\")\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\", low_cpu_mem_usage=True)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ngpt = gpt.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txt_prompt = \"\"\"Genre: Science Fiction.\n###\nScene: : desert with floating ice-creams.\n###\nStory plot: Mr Bean has to collect 100 ice cream flavors within a day to open a portal to the mothership.\n###\nCharacter1: Cyberpunk Snow White princess.\n###\nCharacter2: Mr. Bean as a Star Wars character.\n###\nScript:\n###\nBean:\"\"\" ","metadata":{"execution":{"iopub.status.busy":"2022-06-26T19:31:49.388084Z","iopub.execute_input":"2022-06-26T19:31:49.388859Z","iopub.status.idle":"2022-06-26T19:31:49.393695Z","shell.execute_reply.started":"2022-06-26T19:31:49.388818Z","shell.execute_reply":"2022-06-26T19:31:49.392361Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"prompt = tokenizer(txt_prompt, return_tensors='pt')\nprompt = {key: value.to(device) for key, value in prompt.items()}\nout = gpt.generate(**prompt, min_length=256, max_length=256, do_sample=True)\nprint(tokenizer.decode(out[0]))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T19:31:50.044150Z","iopub.execute_input":"2022-06-26T19:31:50.044865Z","iopub.status.idle":"2022-06-26T19:32:24.292410Z","shell.execute_reply.started":"2022-06-26T19:31:50.044822Z","shell.execute_reply":"2022-06-26T19:32:24.291189Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}